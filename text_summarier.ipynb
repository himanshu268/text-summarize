{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\rajpu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "nltk.download('punkt_tab')\n",
    "import statistics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\rajpu\\AppData\\Local\\Temp\\ipykernel_9292\\1914025808.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  sentence = re.sub('[^a-zA-Z\\s]', \"\", sentence)\n",
      "C:\\Users\\rajpu\\AppData\\Local\\Temp\\ipykernel_9292\\1914025808.py:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  sentence = re.sub('\\s+', \" \", sentence).strip()\n"
     ]
    }
   ],
   "source": [
    "def clean_data(filename):\n",
    "    # Open and read the entire file content\n",
    "    with open(filename, \"r\",encoding='unicode-escape') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Split the content into sentences\n",
    "    article = re.split(r'\\.\\s+', filedata)\n",
    "\n",
    "    # Initialize an empty list to hold cleaned sentences\n",
    "    sent = []\n",
    "\n",
    "    for sentence in article:\n",
    "        # Remove non-alphabetic characters and extra spaces\n",
    "        sentence = re.sub('[^a-zA-Z\\s]', \"\", sentence)\n",
    "        sentence = re.sub('\\s+', \" \", sentence).strip()\n",
    "        if sentence:  # Append only non-empty sentences\n",
    "            sent.append(sentence)\n",
    "\n",
    "    # Join the cleaned sentences into a single string\n",
    "    sent.pop()\n",
    "    datas = \" \".join(sent)\n",
    "    print(\"Initial text:\")\n",
    "    print(filedata)\n",
    "    print('\\n')\n",
    "    print(\"Cleaned text:\")\n",
    "    print(datas)\n",
    "    print('\\n')\n",
    "\n",
    "    return sent,filedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(sent):\n",
    "    cnt=0\n",
    "    words=word_tokenize(sent)\n",
    "    for word in words:\n",
    "        cnt+=1\n",
    "    return cnt\n",
    "\n",
    "def cnt_inSent(sent):\n",
    "    txt_data=[]\n",
    "    i=0\n",
    "    for s in sent:\n",
    "        i+=1\n",
    "        data=count(s)\n",
    "        temp={\"id\":i, \"word_cnt\":data}\n",
    "        txt_data.append(temp)\n",
    "    return txt_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dict(sent):\n",
    "    i=0\n",
    "    freq_list=[]\n",
    "    for s in sent:\n",
    "        i+=1\n",
    "        freq={}\n",
    "        words=word_tokenize(s)\n",
    "        for char in words:\n",
    "            char=char.lower()\n",
    "            if char in freq:\n",
    "                freq[char]+=1\n",
    "            else:\n",
    "                freq[char]=1\n",
    "            temp={\"id\":1,\"freq_dict\":freq}\n",
    "        freq_list.append(temp)\n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(txt_data,freq_list):\n",
    "    tf_score=[]\n",
    "    for item in freq_list:\n",
    "        ID=item[\"id\"]\n",
    "        for k in item[\"freq_dict\"]:\n",
    "            temp={\"id\":ID,\"key\":k,\"tf_score\":item[\"freq_dict\"][k]/txt_data[ID-1][\"word_cnt\"]}\n",
    "            tf_score.append(temp)\n",
    "    return  tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(txt_data, freq_list):\n",
    "    idf_score = []\n",
    "    cnt = 0\n",
    "    for item in freq_list:\n",
    "        cnt += 1\n",
    "        for k in item[\"freq_dict\"]:\n",
    "            val = sum([k in it[\"freq_dict\"] for it in freq_list])\n",
    "            temp = {\n",
    "                \"id\": cnt,\n",
    "                \"idf_score\": math.log((len(txt_data) / (val + 1)) + 1),  # Ensure valid log value\n",
    "                \"key\": k\n",
    "            }\n",
    "            idf_score.append(temp)\n",
    "    return idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfXidf(idf_score,tf_score):\n",
    "    tf_idf_score=[]\n",
    "    for j in idf_score:\n",
    "        for i in tf_score:\n",
    "            if j[\"key\"]==i[\"key\"] and j[\"id\"]==i[\"id\"]:\n",
    "                temp={\n",
    "                    \"id\":j[\"id\"],\n",
    "                    \"tfXidf\":i[\"tf_score\"]*j[\"idf_score\"],\n",
    "                    \"key\":j[\"key\"]\n",
    "                }\n",
    "                tf_idf_score.append(temp)\n",
    "    return tf_idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(tf_idf_score,sent,txt_data):\n",
    "    # calculate the ranking of the sentence\n",
    "    sent_data=[]\n",
    "    for txt in txt_data:\n",
    "        score=0\n",
    "        for i in range(len(tf_idf_score)):\n",
    "            t_dict=tf_idf_score[i]\n",
    "            if txt[\"id\"]==t_dict[\"id\"]:\n",
    "                score+=t_dict[\"tfXidf\"]\n",
    "        temp={\n",
    "            \"id\":txt[\"id\"],\n",
    "            \"score\":score,\n",
    "            \"sentence\":sent[txt[\"id\"]-1]\n",
    "        }\n",
    "        sent_data.append(temp)\n",
    "    return sent_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(sent_data):\n",
    "    cnt=0\n",
    "    summary=[]\n",
    "    arr=[]\n",
    "    for t_dict in sent_data:\n",
    "        cnt+=t_dict['score']\n",
    "    avg=cnt/len(sent_data)\n",
    "    for  temp in sent_data:\n",
    "        arr.append(temp['score'])\n",
    "    stdev=statistics.stdev(arr)\n",
    "\n",
    "    for sent in sent_data:\n",
    "        if sent['score'] >=(avg + 3*stdev):\n",
    "            summary.append(sent[\"sentence\"])\n",
    "\n",
    "\n",
    "    summary=\" \".join(summary)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz=Tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def complete_summary(sent_data, incomplete_summary):\n",
    "    tz.fit_on_texts([sent_data])\n",
    "    length = len(tz.word_index) + 1\n",
    "    input_seq = []\n",
    "    \n",
    "    # Generating input sequences\n",
    "    for sent in sent_data.split(\" \"):\n",
    "        token_sent = tz.texts_to_sequences([sent])[0]\n",
    "        for i in range(1, len(token_sent)):\n",
    "            input_seq.append(token_sent[:i+1])\n",
    "    \n",
    "    max_len = max([len(x) for x in input_seq])\n",
    "    padded_input_sequences = pad_sequences(input_seq, maxlen=max_len, padding='pre')\n",
    "    X = padded_input_sequences[:, :-1]\n",
    "    y = padded_input_sequences[:, -1]\n",
    "    y = to_categorical(y, num_classes=length)\n",
    "    z = X.shape[1]\n",
    "    \n",
    "    # Model definition\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=length, output_dim=100, input_length=z))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(length, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Model training\n",
    "    model.fit(X, y, epochs=100)\n",
    "    \n",
    "    last = incomplete_summary[len(incomplete_summary) - 1]\n",
    "    arr = [incomplete_summary]\n",
    "    \n",
    "    # Predicting new words\n",
    "    while True:\n",
    "        token_text = tz.texts_to_sequences([incomplete_summary])[0]\n",
    "        padded_token_text = pad_sequences([token_text], maxlen=max_len, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        pos = np.argmax(model.predict(padded_token_text))\n",
    "        \n",
    "        # Find the word corresponding to the predicted index\n",
    "        for word, index in tz.word_index.items():\n",
    "            if index == pos:\n",
    "                last = word\n",
    "                arr.append(word)\n",
    "                print(word)\n",
    "                \n",
    "                # Exit the loop if the predicted word is a period (\".\")\n",
    "                if word == \".\":\n",
    "                    break\n",
    "                \n",
    "                # Add the new word to the incomplete summary for the next prediction\n",
    "                incomplete_summary += \" \" + word\n",
    "        \n",
    "        # Break the outer loop if the word is \".\"\n",
    "        if last == \".\":\n",
    "            break\n",
    "    \n",
    "    # Return the completed summary\n",
    "    completed_summ = \" \".join(arr)\n",
    "    return completed_summ\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "The flickering streetlight outside cast shadows on the walls, dancing to the rhythm of the autumn wind. The city was quiet, save for the occasional hum of distant traffic and the whispers of leaves brushing against the pavement.\n",
      "\n",
      "It had been five years since Samira left her homeland, a small village nestled in the mountains, for the sprawling metropolis where opportunities were supposed to be endless. Back then, she had dreams of building a life that her family could be proud of. But the city, with its cold towers and relentless pace, had a way of swallowing dreams whole. The promises of success turned into long hours at a job that drained her spirit, far from the warmth and familiarity of home.\n",
      "\n",
      "Tonight was different, though. Tonight, she was going back.\n",
      "\n",
      "Samiraâs phone buzzed with a notification, pulling her out of her thoughts. It was from her mother, a short but heartfelt message, âWeâre waiting for you.â Her heart swelled with emotion. It had been too long since she had seen her family, felt the embrace of her mother, or heard the laughter of her younger brother echo through the house. She had missed weddings, festivals, and countless family dinners, all for the sake of chasing a dream that felt more distant every day.\n",
      "\n",
      "As she packed her final belongings into a small suitcase, memories of her village flooded back. She remembered the narrow, winding roads that led to her house, the smell of fresh bread baking in the morning, and the sound of children playing in the fields. The city had nothing like that. Here, everything was steel, glass, and concrete â cold and indifferent.\n",
      "\n",
      "Samira took one last look around her apartment. The space was sparse, with only the essentials: a bed, a desk, a few books scattered on the floor. It had been her refuge in the city, but it had never felt like home. Home was where the mountains touched the sky, where the air was crisp, and where every face she passed on the street carried a story she knew by heart.\n",
      "\n",
      "The journey back wasnât just about returning to a place. It was about reclaiming a part of herself that she had left behind. Samira had changed over the years. The city had hardened her, but it had also taught her resilience. She had learned to navigate its complexities, but in doing so, she had lost touch with the simplicity and beauty of life back home.\n",
      "\n",
      "As she stepped out into the cool night, the weight of the city seemed to lift off her shoulders. She hailed a cab to take her to the train station, the final leg of her journey before she would be back in the embrace of her family.\n",
      "\n",
      "The train ride was long, but Samira didnât mind. With each passing mile, the cityâs lights faded into the distance, replaced by rolling hills and open skies. The farther she traveled, the lighter her heart felt.\n",
      "\n",
      "When the train finally arrived at the station, she saw them â her mother, her brother, and her father, waiting with open arms. Samira smiled, knowing that after all the years, she was finally home.\n",
      "\n",
      "\n",
      "Cleaned text:\n",
      "The flickering streetlight outside cast shadows on the walls dancing to the rhythm of the autumn wind The city was quiet save for the occasional hum of distant traffic and the whispers of leaves brushing against the pavement It had been five years since Samira left her homeland a small village nestled in the mountains for the sprawling metropolis where opportunities were supposed to be endless Back then she had dreams of building a life that her family could be proud of But the city with its cold towers and relentless pace had a way of swallowing dreams whole The promises of success turned into long hours at a job that drained her spirit far from the warmth and familiarity of home Tonight was different though Tonight she was going back Samiras phone buzzed with a notification pulling her out of her thoughts It was from her mother a short but heartfelt message Were waiting for you Her heart swelled with emotion It had been too long since she had seen her family felt the embrace of her mother or heard the laughter of her younger brother echo through the house She had missed weddings festivals and countless family dinners all for the sake of chasing a dream that felt more distant every day As she packed her final belongings into a small suitcase memories of her village flooded back She remembered the narrow winding roads that led to her house the smell of fresh bread baking in the morning and the sound of children playing in the fields The city had nothing like that Here everything was steel glass and concrete cold and indifferent Samira took one last look around her apartment The space was sparse with only the essentials a bed a desk a few books scattered on the floor It had been her refuge in the city but it had never felt like home Home was where the mountains touched the sky where the air was crisp and where every face she passed on the street carried a story she knew by heart The journey back wasnt just about returning to a place It was about reclaiming a part of herself that she had left behind Samira had changed over the years The city had hardened her but it had also taught her resilience She had learned to navigate its complexities but in doing so she had lost touch with the simplicity and beauty of life back home As she stepped out into the cool night the weight of the city seemed to lift off her shoulders She hailed a cab to take her to the train station the final leg of her journey before she would be back in the embrace of her family The train ride was long but Samira didnt mind With each passing mile the citys lights faded into the distance replaced by rolling hills and open skies The farther she traveled the lighter her heart felt When the train finally arrived at the station she saw them her mother her brother and her father waiting with open arms\n",
      "\n",
      "\n",
      "Summary:\n",
      "The flickering streetlight outside cast shadows on the walls dancing to the rhythm of the autumn wind.\n"
     ]
    }
   ],
   "source": [
    "# Example of correct function calls assuming tf and idf are values\n",
    "file,clean_text = clean_data(r\"G:\\text_summarier-main\\text_summarier-main\\text.txt\")\n",
    "text_data = cnt_inSent(file)\n",
    "freq = freq_dict(file)\n",
    "\n",
    "# Ensure tf and idf are calculated and not called as functions\n",
    "tf_scores = calculate_tf(text_data, freq)\n",
    "idf_scores = calculate_idf(text_data, freq)\n",
    "\n",
    "# Multiplying TF and IDF\n",
    "tfidf_scores = tfXidf(idf_scores, tf_scores)\n",
    "\n",
    "# Ranking and summarizing\n",
    "rankings = ranking(tfidf_scores, file, text_data)\n",
    "result = summary(rankings)\n",
    "print(\"Summary:\")\n",
    "print(result +\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train finally arrived station, saw mother, brother, father, waiting open arms . long since seen family, felt embrace mother\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "def summarize_text(text, num_sentences=3):\n",
    "    # Clean the text\n",
    "    clean_text = clean_and_tokenize(text)\n",
    "    \n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(clean_text)\n",
    "    \n",
    "    # Calculate TF-IDF scores\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).ravel()\n",
    "    \n",
    "    # Rank sentences by score\n",
    "    ranked_sentences = [sent for sent, score in sorted(zip(sentences, sentence_scores), key=lambda x: x[1], reverse=True)]\n",
    "    \n",
    "    # Extract top sentences\n",
    "    extractive_summary = \" \".join(ranked_sentences[:num_sentences])\n",
    "    \n",
    "    # Optionally, use BERT for refinement\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    abstractive_summary = summarizer(extractive_summary, max_length=27, min_length=20, do_sample=False)\n",
    "    \n",
    "    return abstractive_summary[0]['summary_text']\n",
    "\n",
    "# Example usage\n",
    "text = clean_text\n",
    "summary = summarize_text(text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
